---
title: "Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan"
subtitle: "Take-home_Ex02"
format:
  html:
    code-fold: false
    code-summary: "Show the code"
    toc: true # Table of Contents
execute:
  eval: true
  echo: true
  freeze: true
  warning: False  # This turns off warning messages being displayed
date: 19 February, 2024
date-modified: "last-modified"
---

# 1.0 Introduction

## 1.1 Overview

[Dengue Hemorrhagic Fever](https://www.cdc.gov/dengue/resources/denguedhf-information-for-health-care-practitioners_2009.pdf) (in short dengue fever) is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions. It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes. In 2015, Taiwan had recorded the most severe dengue fever outbreak with more than 43,000 dengue cases and 228 deaths. Since then, the annual reported dengue fever cases were maintained at the level of not more than 200 cases. However, in 2023, Taiwan recorded 26703 dengue fever cases.

## 1.2 Objectives

As a curious geospatial analytics green horn, you are interested to discover:

-   if the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.

-   If the outbreak is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.

## 1.3 The Task

The specific tasks of this take-home exercise are as follows:

-   Using appropriate function of **sf** and **tidyverse**, preparing the following geospatial data layer:

    -   a study area layer in sf polygon features. It must be at village level and confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.

    -   a dengue fever layer within the study area in sf point features. The dengue fever cases should be confined to epidemiology week 31-50, 2023.

    -   a derived dengue fever layer in [spacetime s3 class of sfdep](https://sfdep.josiahparry.com/articles/spacetime-s3). It should contain, among many other useful information, a data field showing number of dengue fever cases by village and by epidemiology week.

-   Using the extracted data, perform global spatial autocorrelation analysis by using [sfdep methods](https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex05/in-class_ex05-glsa).

-   Using the extracted data, perform local spatial autocorrelation analysis by using [sfdep methods](https://r4gdsa.netlify.app/chap10.html).

-   Using the extracted data, perform emerging hotspot analysis by using [sfdep methods](https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex05/in-class_ex05-ehsa).

-   Describe the spatial patterns revealed by the analysis above.

# 2.0 Packages

The packages used in this project are:

-   **sf:** for importing, managing, and processing geospatial data

-   **tidyverse**: a family of R packages for performing data science tasks such as importing, wrangling, and visualizing data

-   **tmap:** creating thematic maps

-   **sfdep:** for analyzing spatial dependencies

```{r}
pacman::p_load(sf, tidyverse, tmap, sfdep, dplyr, zoo)
```

# 3.0 Data Acquisition

For the purpose of this take-home exercise, two data sets are provided, they are:

+---------------------+----------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| Data                | Format   | Description                                                                                                               | Source                                                                                                              |
+=====================+==========+===========================================================================================================================+=====================================================================================================================+
| TAIWAN_VILLAGE_2020 | ESRI     | A Geospatial data of village boundary of Taiwan. The data is in Taiwan Geographic Coordinate System.                      | [Historical map data of the village boundary: TWD97 longitude and latitude](https://data.gov.tw/en/datasets/130549) |
+---------------------+----------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| Dengue_Daily.csv    | CSV      | An Aspatial data of reported dengue cases in Taiwan since 1998. Below are selected fields that are useful for this study: | [Dengue Daily Confirmed Cases Since 1998](https://data.cdc.gov.tw/en/dataset/dengue-daily-determined-cases-1998)    |
|                     |          |                                                                                                                           |                                                                                                                     |
|                     |          | -   發病日: Onset date                                                                                                    |                                                                                                                     |
|                     |          |                                                                                                                           |                                                                                                                     |
|                     |          | -   最小統計區中心點X: x-coordinate                                                                                       |                                                                                                                     |
|                     |          |                                                                                                                           |                                                                                                                     |
|                     |          | -   最小統計區中心點Y: y-coordinate                                                                                       |                                                                                                                     |
+---------------------+----------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+

# 4.0 Data Wrangling

Can reference In-Class_Ex05

## 4.1 Importing Taiwan Boundary dataset

Let's see what layers the dataset has.

```{r}
file_path = "../../data/geospatial/TAIWAN_VILLAGE_2020"
layers <- st_layers(file_path)
print(layers)
```

Since it only has one "TAINAN_VILLAGE" layer, lets pull it.

```{r}
taiwan_sf <- st_read(dsn="../../data/geospatial/TAIWAN_VILLAGE_2020", layer="TAINAN_VILLAGE")
```

Take note that this dataset is in TWD97 Geographic Coordinate System.

### 4.1.1 Extracting Polygon Study Area

```{r}
head(taiwan_sf,1)
```

We are interested in the "TOWNID" column. Specifically the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.

```{r}
# Assuming taiwan_sf is your imported sf object
study_area_sf <- taiwan_sf %>%
  filter(TOWNID %in% c("D01", "D02", "D04", "D06", "D07", "D08", "D32", "D39"))

# Check the result
print(unique(study_area_sf$TOWNID))
```

### 4.1.2 Drop Unnecessary Columns

Let's drop the "NOTE" column from the dataset

```{r}
# Select only the necessary columns
study_area_sf <- study_area_sf %>%
  select(VILLCODE, COUNTYNAME, TOWNNAME, VILLNAME, VILLENG, COUNTYID, COUNTYCODE, TOWNID, TOWNCODE, geometry)

# Check the result
print(study_area_sf)
```

## 4.2 Importing Dengue Daily dataset

This dataset contains the dengue cases in 2020 and is in TWD97 Geographic Coordinate System.

The data is in a geographic coordinate system with longitude and latitude, even though it is in decimal. It is not Projected Coordinate System.

```{r}
dengue2023 <- read_csv("../../data/aspatial/DengueDaily/Dengue_Daily.csv")
```

```{r}
summary(dengue2023)
```

### 4.2.1 Extract Study Area

```{r}
# First, convert '發病日' to Date format
dengue2023 <- dengue2023 %>%
  mutate(發病日期 = ymd(發病日))

# Then, add a column for the epidemiological week and year
dengue2023 <- dengue2023 %>%
  mutate(
    EPID_WEEK = isoweek(發病日期),
    EPID_YEAR = year(發病日期)
  )

# Now, filter the data for epidemiological weeks 31-50 in 2023
dengue_week31_50_2023 <- dengue2023 %>%
  filter(EPID_YEAR == 2023 & EPID_WEEK >= 31 & EPID_WEEK <= 50)

# Check the result
print(dengue_week31_50_2023)
```

### 4.2.2 Drop Unnecessary Columns

In the Data Acquisition section, we mentioned that we only need these columns for the analysis:

-   發病日: Onset date

    -   Since in the above processing, we created a new field called (發病日期) with the modified date object, we will use that instead.

-   最小統計區中心點X: x-coordinate

-   最小統計區中心點Y: y-coordinate

-   EPID_WEEK: Epidemiology Week

Let's keep only these 4 columns:

```{r}
# Select only the necessary columns
dengue_cases <- dengue_week31_50_2023 %>%
  select(發病日期, 最小統計區中心點X, 最小統計區中心點Y, EPID_WEEK)

# Check the result
print(dengue_cases)
```

### 4.2.3 Convert Coordinates to numeric values

I noticed that the coordinates are in 'chr' character strings. We need them to be in numeric.

```{r}
dengue_cases <- dengue_cases %>%
  mutate(
    最小統計區中心點X = as.numeric(最小統計區中心點X),
    最小統計區中心點Y = as.numeric(最小統計區中心點Y)
  ) %>%
  na.omit()
```

## 4.3 Combining and Aggregating the data

### 4.3.1 Spatial Join

Perform a spatial join between the **`dengue_cases`** points and the **`study_area_sf`** polygons to determine which dengue cases occur in which village.

```{r}
# Step 1: Spatial Join
# Convert dengue_cases to an sf object by using the coordinates
dengue_cases_sf <- st_as_sf(dengue_cases, coords = c("最小統計區中心點X", "最小統計區中心點Y"), crs = st_crs(study_area_sf))

# Perform the spatial join
study_area_cases_sf <- st_join(study_area_sf, dengue_cases_sf) %>%
  na.omit()
```

### 4.3.2 Aggregation by VILLENG

Aggregate the cases by the 'VILLENG' (village name)

```{r}
# Step 3: Aggregation
# Aggregate by village name (VILLENG) and epidemiological week
study_area_aggregated <- study_area_cases_sf %>%
  group_by(VILLENG) %>%
  summarise(CASE_COUNT = n())
```

## 4.4 Visualizing Data

Draw a choropleth map

```{r}
equal <- tm_shape(study_area_aggregated) +
  tm_fill("CASE_COUNT",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval classification")

quantile <- tm_shape(study_area_aggregated) +
  tm_fill("CASE_COUNT",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile classification")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)
```

## 4.5 Spacetime Object

```{r}
#| eval: false
# Step 3: Aggregation
# Aggregate by village name (VILLENG) and epidemiological week
study_area_time_aggregated <- study_area_cases_sf %>%
  group_by(VILLENG, EPID_WEEK) %>%
  summarise(CASE_COUNT = n())
```

### 4.5.1 Expanding the data

Our aggregated dataset only stores the data of cases that occur. However, there will be location and times where no cases occur. We need to expand our data to include those rows with CASE_COUNT values of 0. This is to make the spacetime object creation smoother.

```{r}
#| eval: false
# Create a full grid of location and time period combinations
full_grid <- expand.grid(
  VILLENG = unique(study_area_sf$VILLENG),
  EPID_WEEK = unique(study_area_time_aggregated$EPID_WEEK)
)

# Join the grid with spatial data to ensure geometry is present for all villages
full_grid_with_geom <- left_join(full_grid, study_area_sf %>% select(VILLENG, geometry), by = "VILLENG")

# Join the spatially enriched grid with case count data
complete_data <- left_join(full_grid_with_geom, study_area_aggregated, by = c("VILLENG", "EPID_WEEK"))

# Replace NA values in CASE_COUNT with zero 
complete_data$CASE_COUNT[is.na(complete_data$CASE_COUNT)] <- 0

# Convert back to an sf object 
complete_data <- complete_data %>%
  select(-geometry.x) %>%
  rename(geometry = geometry.y)

complete_data_sf <- st_as_sf(complete_data, sf_column_name = "geometry")

```

```{r}
#| eval: false
# Assuming 'complete_data_sf' is your prepared sf object before attempting to create the spacetime cube

# Aggregate to ensure one observation per location-time combination
# Replace 'CASE_COUNT' with your actual case count column, if named differently
aggregated_data <- complete_data_sf %>%
  group_by(VILLENG, EPID_WEEK) %>%
  summarize(CASE_COUNT = sum(CASE_COUNT), .groups = 'drop')

# Ensure the aggregated data is still an sf object
aggregated_data_sf <- st_as_sf(aggregated_data, sf_column_name = "geometry", crs = st_crs(complete_data_sf))

# Try creating the spacetime object again
dengue_st <- spacetime(
  aggregated_data_sf, 
  study_area_sf, 
  "VILLENG", 
  "EPID_WEEK"
)

# Attempt to complete the spacetime cube
dengue_st_cube <- complete_spacetime_cube(dengue_st)
```

### 4.5.2 Spacetime Cube Object Creation

Create an spacetime cube object using **`sfdep`** package, which will allow for the analysis of spatial and spatio-temporal patterns.

```{r}
#| eval: false
dengue_st <- spacetime(
  complete_data_sf, 
  study_area_sf, 
  "VILLENG", 
  "EPID_WEEK")
```

```{r}
#| eval: false
is_spacetime_cube(dengue_st)
```

Seems like the in certain locations, case_count might have missing values since there could possible be no cases in that location and time period. Let's fill the missing values with NA using the complete_spacetime_cube() function.

```{r}
#| eval: false
# Assuming dengue_st is your current spacetime object
dengue_st_cube <- complete_spacetime_cube(dengue_st)

# Check if the object is now a valid spacetime cube
is_spacetime_cube(dengue_st_cube)
```

# 5.0 Global Spatial Autocorrelation

Global Spatial Autocorrelation measures the overall degree of spatial dependence within a geographic area. The most common statistic for measuring global spatial autocorrelation is Moran's I. This statistic compares the value of spatial data at one location with values at neighboring locations over the entire study area to determine if there is a significant pattern. If Moran's I is positive and significant, it suggests a clustered pattern; if it is negative and significant, it suggests a dispersed pattern; if it is close to zero, it indicates a random spatial pattern.

Global Autocorrelation Analysis:

-   Calculate Moran's I for the entire study area

-   Assess the significance of the statistic to determine if there is a pattern

-   Interpret the result to understand the overall tendency of data clustering or dispersion

## 5.1 Computing Contiguity Spatial Weights

Before we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.

We will derive the contiguity weights using the **Queen's method**. (Similar to the way a Queen moves in chess, we will take into account neighbors in all principal directions - cardinal and intercardinal)

```{r}
wm_q <- study_area_aggregated %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb,
                         style = "W"),
         .before = 1)
```

```{r}
wm_q
```

## 5.2 Performing Global Moran's I Test

Let's run the Global Moran's I test.

```{r}
global_moran_test(wm_q$CASE_COUNT,
                  wm_q$nb,
                  wm_q$wt)
```

## 5.3 Performing Global Moran's I Permutation Test

We will apply Monte Carlo simulation to perform the statistical test 100 times.

First, we set a seed to maintain the reproduceability of the results.

```{r}
set.seed(8888)
```

```{r}
global_moran_perm(wm_q$CASE_COUNT,
                  wm_q$nb,
                  wm_q$wt,
                  nsim = 100)
```

The statistical report shows that the p-value is smaller than alpha value 0.05.

Therefore, we have enough evidence to **reject** the null hypothesis that the **spatial distribution of cases resembles random distribution**.

Since Moran's I statistics is greater than 0, we can infer that the **spatial distribution show signs of clustering**.

# 6.0 Local Spatial Autocorrelation

Local Spatial Autocorrelation breaks down the global statistic into contributions from each individual location, allowing for the assessment of where and how much each feature contributes to the overall autocorrelation statistic. Local indicators of spatial association (LISA), such as Local Moran's I, allow for the identification of clusters or outliers where a feature has a value significantly different from its neighbors. This local approach helps to pinpoint specific areas of interest within the global context.

Local Autocorrelation Analysis:

-   Calculate Local Moran's I for each feature in the dataset

-   Create a significance map that highlights areas of significant local clustering or outliers

-   Use this detailed view to identify hot spots (high values surrounded by high values), cold spots (low values surrounded by low values), and spatial outliers (high values surrounded by low values or vice versa)

## 6.1 Computing Local Moran's I

```{r}
lisa <- wm_q %>% 
  mutate(local_moran = local_moran(
    CASE_COUNT, nb, wt, nsim = 99),
         .before = 1) %>%
  unnest(local_moran)
```

The output of local_moran() is a sf data.frame containing some of the following columns:

+----------------+-------------------------------------------------------------------------+
| Column         | Description                                                             |
+================+=========================================================================+
| ii             | Local Moran statistic                                                   |
+----------------+-------------------------------------------------------------------------+
| eii            | Expected Local Moran statistic; or the permutation sample mean          |
+----------------+-------------------------------------------------------------------------+
| var_ii         | Variance of Local Moran statistic; or the permutation sample deviations |
+----------------+-------------------------------------------------------------------------+
| z_ii           | Standard deviate of Local Moran statistic                               |
+----------------+-------------------------------------------------------------------------+
| p_ii           | p-value of Local Moran statistic using pnorm()                          |
+----------------+-------------------------------------------------------------------------+

## 6.2 Visualizing Local Moran's I

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of Case Count",
            main.title.size = 0.8)
```

## 6.3 Visualizing p-value of local Moran's I

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("p_ii_sim") + 
  tm_borders(alpha = 0.5) +
   tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)
```

## 6.4 Visualizing LISA map

The LISA map will categorize each region to show the outliers and clusters.

Types of clusters and outliers:

| Clusters  | Outliers |
|-----------|----------|
| High-High | High-Low |
| Low-Low   | Low-High |

In the lisa sf data.frame, there are 3 fields: mean, median and pysal. In general, classification in mean will be used.

```{r}
lisa_sig <- lisa  %>%
  filter(p_ii < 0.05)
tmap_mode("plot")
tm_shape(lisa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(lisa_sig) +
  tm_fill("mean") + 
  tm_borders(alpha = 0.4)
```

# 7.0 Emerging Hot Spot and Cold Spot Area Analysis (HCSA)

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time.

Emerging Hot Spot Analysis:

-   Build a spacetime cube

-   Calculate Getis-Ord local Gi\* statistic for each bin by using an FDR correction

-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test

-   Categorizing each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.

## 7.1 Computing Local Gi\* statistics

```{r}

```

# Global Measures of Spatial Association (Ex05)

[is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex05/in-class_ex05-glsa](https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex05/in-class_ex05-glsa)

Deriving contiguity weights: Queen's method

Then draw a bunch of choropleth map

when using sfdep, the wm_q will be saved as dataframe. This means we can just look at the neighbors by looking directly at the dataset (like the in-class exercise), instead of only being able to look at it after printing (like the hands-on exercise)

Moran I test statistics: use p-value to compare with critical value to reject or accept null hypothesis. (usually using 95% -\> alpha value = 0.05 or 90% -\> alpha value = 0.1)

-   Reject Null Hypothesis if p-value \< critical value -\> inferred .... represents clustering

Is there a spacial bias-ness? Test its distribution (like Complete Spatial Randomness, or does it have some clustering). The null hypothesis is that it resembles CSR.

This stuff is called Frequentist test because we only take one result. But in the modern data science world, we don't use this because it's not enough, instead we use monte-carlo simulation to test it multiple times. For this assignment, we can just use monte carlo.

global_moran_perm(wm_q\$GDPPC, wm_q\$nb, wm_q\$wt, nsim = 99)

Correct method to use is always permutation method.

MOVING ON

local moran is a decomposition of moran, so we can detect specific cluster outliers

-   Outlier =\> different from neighbors (high-low, low-high)

-   Clusters =\> similar to neighbors (high-high, low-low)

We can plot the p-value for each region

lisa_sig \<- lisa %\>% filter(p_ii \< 0.05) tmap_mode("plot") .......................

MOVING ON

Gi\* statistics, it needs to be in distance based metrics

Gi\* statistics defines everything as a cluster and tells us if its a hotspot or coldspot cluster

can use fixed distance, adaptive distance or inverse distance (like in-class ex)

The graph from GI will have positive and negative

-   positive = hotspot

-   negative = coldspot

MOVING ON

we can combine both, the statistically significant areas from local moran and the hotspot/coldspot from Gi\* statistics

FOR THIS TAKE HOME ASSIGNMENT, we need to decide which one to use, and see if we need both or just one

MOVING ON

SPATIAL TIME CUBE

each cube represents one geographical area.

# Local Measures of Spatial Association

# Emerging Hotspot Analysis
